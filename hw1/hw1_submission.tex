\documentclass{article}
\usepackage{xcolor}
\usepackage{titleps}
\usepackage[letterpaper, margin=0.95in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{tabu}
\usepackage{parskip}
\usepackage{natbib}
\usepackage{listings}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{wrapfig}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{makecell}
% % Support for easy cross-referencing
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{colortbl}
\usepackage{xcolor}
% \usepackage{epsfig} % for postscript graphics files
\usepackage{empheq}
%\usepackage{mathptmx} % assumes new font selection scheme installed
% \usepackage{times} % assumes new font selection scheme installed
\usepackage{bm}
\usepackage{bbding} 
% \usepackage{cite}
\usepackage{diagbox}
\usepackage[linesnumbered,ruled]{algorithm2e}
% \usepackage{ulem} %to strike the words
% \usepackage{hyperref}
% \usepackage{soul}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \definecolor{themeblue}{RGB}{57, 162, 219}
% \definecolor{themegreen}{RGB}{87, 204, 153}
% \definecolor{forestgreen}{RGB}{47, 159, 87}

\usepackage[capitalize]{cleveref}
% \usepackage{todonotes}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{ bbold }
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
%\usepackage{subfigure}
\usepackage{pifont}
\usepackage{threeparttable}

\usepackage{hyperref}
\usepackage[color=red]{todonotes}
\usepackage{forest}
\definecolor{light-yellow}{HTML}{FFE5CC}
\newcounter{RNum}
\renewcommand{\theRNum}{\arabic{RNum}}
\newcommand{\Remark}{\noindent\textbf{Remark}~\refstepcounter{RNum}\textbf{\theRNum}: }
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\highlight}[1]{\noindent\quad\textbf{#1}:~}
\newcommand{\myparagraph}[1]{\noindent\textbf{#1}~}

\newpagestyle{ruled}
{\sethead{CMU 16-831}{Intro to Robot Learning}{Fall 2025}\headrule
  \setfoot{}{}{}}
\pagestyle{ruled}

\renewcommand\makeheadrule{\color{black}\rule[-.75\baselineskip]{\linewidth}{0.4pt}}
\renewcommand*\footnoterule{}

\begin{document}
\lstset{basicstyle = \ttfamily,columns=fullflexible,
backgroundcolor = \color{light-yellow}
}

\begin{centering}
    {\Large Assignment 1: Imitation Learning} \\
    \vspace{.25cm}
    \textbf{Andrew ID:} \texttt{bharathh} \\
    \textbf{Collaborators:} \texttt{None}\\ 
\end{centering}

\vspace{.5cm}

\section{Behavioral Cloning (9.75 pt)}
\subsection{Part 2 (1.5 pt)}
\begin{table}[!h]
  \centering
  \caption{Mean and standard deviation of return over two trajectories of the expert data for each environment.}
    \begin{tabular}{cccccc}
    \toprule[1.0pt]
    Metric/Env & Ant-v2 & Humanoid-v2 & Walker2d-v2 & Hopper-v2 & HalfCheetah-v2 \\
    \midrule
    Mean  & 4713.65 & 10344.52 & 5566.85 & 3772.67 & 4205.78 \\
    Std.  & 12.20 & 20.98 & 9.24 & 1.95 & 83.04 \\
    \bottomrule[1.0pt]
    \end{tabular}%
  \label{tab:p2}%
\end{table}%

\subsection{Part 3 (5.25 pt)}
\begin{table}[htbp]
  \centering
  \caption{Hyperparameters: n\_layers=5, learning\_rate=3e-3, eval\_batch\_size=5000; rest are default values. BC Agent reaches 47.85\% and 7.99\% of the expert performance for Ant-v2 and Hopper-v2 environments respectively.}
    \begin{tabular}{ccccc}
    \toprule[1.0pt]
    Env   & \multicolumn{2}{c}{Ant-v2} & \multicolumn{2}{c}{Hopper-v2} \\
    \midrule
    Metric & Mean  & Std.  & Mean  & Std. \\
    Expert & 4713.65 & 12.20 & 3772.67 & 1.95 \\
    BC    & 2255.59 & 1699.11 & 301.59 & 359.24 \\
    \bottomrule[1.0pt]
    \end{tabular}%
  \label{tab:p3}%
\end{table}%

\subsection{Part 4 (3 pt)}
\begin{figure*}[!h]
	\centering
\includegraphics[width=0.7\columnwidth]{graphs/learning_rate_sweep.png}
	\caption{The graph shows mean return $\pm$ standard deviation of the BC Agent in Ant-v2 environment over approximately five rollouts for each learning rate value: 1e-4, 5e-4, 1e-3, 5e-3, 1e-2 and 5e-2.
  Rest of the hyperparameters are same as in Table 2. 
  Learning rate was chosen because it controls the gradient step size and can impact both convergence speed and final performance, as can be seen from the graph.
  }
	\label{fig:p4}
\end{figure*}
\newpage
\section{DAgger (5.25 pt)}
\subsection{Part 2 (5.25 pt)}
\begin{figure*}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{graphs/dagger_learning_curves.png}
	\caption{DAgger learning curves for Ant-v2 (left) and Hopper-v2 (right) environments.
  The graphs show mean return $\pm$ standard deviation over approximately five rollouts for each DAgger iteration. 
  Hyperparameters: n\_layers=3, n\_iter=8, eval\_batch\_size=5000; rest are default values.
  As seen, DAgger outperforms BC agent, reaching 99.8\% of the expert performance in both environments.}
	\label{fig:p5}
\end{figure*}

\end{document}
